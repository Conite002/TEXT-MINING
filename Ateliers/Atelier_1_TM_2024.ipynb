{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K-VU9WD0yYn"
   },
   "source": [
    "# Atelier 1 : Techniques NLP de Base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0v_j4N30yYo"
   },
   "source": [
    "# 1.\tObjectif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eShrsBcZ0yYp"
   },
   "source": [
    "L’objectif de cet atelier est d’apprendre les tâches NLP les plus courantes à travers l’utilisation des bibliothèques nltk, scikitlearn et Spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om75wzht0yYp"
   },
   "source": [
    "# 2.\tOutils et environnement de travail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZIle89Q1Z3m"
   },
   "source": [
    " Installer les package nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kokDFFNE1oz6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnA0rBpZ0yYq"
   },
   "source": [
    "# 3.\tSegmentation (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYVYZdGf0yYq"
   },
   "source": [
    "La segmentation de texte et la tâche de subdivision du texte en petites unités qui seront plus simples à traiter et qu’on appelle tokens.\n",
    "La bibliothèque nltk offre à travers le module **tekenize** un certain nombre de tokinzers qui permettent de réaliser la segmentation du texte en fonction de la nature du problème : words tokenizer, regular-expression based tokenizer, sentences based tokinizers, etc. Ci-dessous une liste non exhaustive de quelques fonctions du module tokinize.\n",
    "\n",
    "* regexp_span_tokenize(text, regexp): Retourne les tokens de texte qui correspondent à l’expression régulière regexp\n",
    "\n",
    "* sent_tokenize(text[, language]):\tRetourne les phrases contenues dans le texte en utilisant le tokenizer PunktSentenceTokenizer.\n",
    "\n",
    "* word_tokenize(text[, language]:\tRetourne les mots contenus dans le texte en utilisant le tokenizer TreebankWordTokenizer avec PunktSentenceTokenizer.\n",
    "\n",
    "NLTK offre également un certain nombre de classes qui offrent des tokinizers plus avancés : BlanklineTokenizer, MWETokenizer, PunktSentenceTokenizer, TextTilingTokenizer, TweetTokenizer, etc.\n",
    "Ci-dessous deux exemples de tokenization à base de **sent_tokenize** et **word_tokenize**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8IEwKpmf0yYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences:  ['Hello, i am very happy to meet you.', 'I created this course for you.', 'Good by!']\n",
      "Words:  ['Hello', ',', 'i', 'am', 'very', 'happy', 'to', 'meet', 'you', '.', 'I', 'created', 'this', 'course', 'for', 'you', '.', 'Good', 'by', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "\n",
    "sentences=sent_tokenize(data)\n",
    "print(\"sentences: \" , sentences)\n",
    "\n",
    "words=word_tokenize(data)\n",
    "print(\"Words: \",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czBN4jcy0yYr"
   },
   "source": [
    "# 4.\tNettoyage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fsYyT9-0yYs"
   },
   "source": [
    "Le nettoyage des données texte joue un rôle très important dans l’amélioration des performances des opérations d’analyse et de découverte de paternes. Ça consiste à la suppression des termes non significatifs **\"Stop words\"**, comme par exemple « le », « la », « de », « du », « ce »… en français et « as » « the », « a », « an », « in » en anglais.  Ces termes qui sont présents fréquemment dans des documents texte peuvent influencer négativement sur la qualité des résultats d’analyse.\n",
    "Le nettoyage peut consister également à la supression des caratères de pontuation et des chaînes de caractères non alphabétiques.\n",
    "Ci-dessous le code qui permet de supprimer les stop words à partir d’un texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6fC3eZk-0yYs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'happy', 'meet', 'created', 'course', 'good']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "data_clean = [word for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF38c0q_0yYs"
   },
   "source": [
    "# 5.\tRacinisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGtnzZea0yYs"
   },
   "source": [
    "La racinisation (Stemming en anglais) permet de normaliser la représentation des mots contenus dans une expression texte en extrayant leurs racines. Ça permettra de supprimer toutes les redondances des mots ayant la même racine. Plusieurs **stemmers** sont offerts par nltk dont les plus utilisés sont : *PorterStemmer, LancasterStemmer, SnowballStemmer...*. Également, le module nltk.stem.snowball offre un certain nombre de stemmers personnalisés à chaque langue, comme par exemple :  *FrenchStemmer, ArabicStemmer*, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuBnt__I0yYt",
    "outputId": "8cb778fc-ca3c-44d5-98a4-ad85c3fe172b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'happi', 'meet', 'creat', 'cours', 'good']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#stemmer=SnowballStemmer('french')\n",
    "stemmer=PorterStemmer()\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "for i in range(len(word_tokens)):\n",
    "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8av6WXdk0yYt"
   },
   "source": [
    "# 6.\tLemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wIpQHvG0yYt"
   },
   "source": [
    "A la différence de la racisation qui fournit souvent une représentation non significative et incomplète des mots, la lemmatisation permet d’obtenir les **formes canoniques** des mots contenus dans une expression texte. Ainsi, au lieu de supprimer juste les suffixes et les préfixes des mots pour obtenir leurs racines, la lemmatisation réalise une **analyse morphologique** des mots afin d’extraire leurs formats canoniques.\n",
    "nltk offre le lemmatizer  *WordNetLemmatizer* pour la réalisation des opérations de lemmatisation, mais uniquement pour l’anglais. pour d'autre langues on peut recourir à la bibliotheque **SpaCy**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQeheHd10yYt",
    "outputId": "07cd40bc-cd41-4c97-f3d0-49d43f531165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'big', 'data', 'gross', 'données', 'anglais', 'mégadonnées', 'données', 'massif', 'désigne', 'ressources', 'information', 'dont', 'caractéristiques', 'termes', 'volume', 'vélocité', 'variété', 'imposent', 'utilisation', 'technology', 'méthodes', 'analytiques', 'particulières', 'générer', 'valeur', 'dépassent', 'général', 'capacités', 'seule', 'unique', 'machine', 'nécessitent', 'traitements', 'parallélisés']\n",
      "['big', 'data', 'field', 'treat', 'way', 'analyze', 'systematically', 'extract', 'information', 'otherwise', 'deal', 'data', 'set', 'large', 'complex', 'dealt', 'traditional', 'application', 'software', 'data', 'many', 'case', 'row', 'offer', 'greater', 'statistical', 'power', 'data', 'higher', 'complexity', 'attribute', 'column', 'may', 'lead', 'higher', 'false', 'discovery', 'rate']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmmatizer=WordNetLemmatizer()\n",
    "data1=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "data2=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "\n",
    "words1 = word_tokenize(data1)\n",
    "words1 = [lemmmatizer.lemmatize(word.lower()) for word in words1 if(not word in set(stopwords.words('french')) and  word.isalpha())]\n",
    "print(words1)\n",
    "\n",
    "words2 = word_tokenize(data2)\n",
    "words2 = [lemmmatizer.lemmatize(word.lower()) for word in words2 if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7h4x5d10yYt"
   },
   "source": [
    "#7.\tPOS-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjcN_uWE0yYt"
   },
   "source": [
    "Le pos-tagging permet de réaliser une analyse lexicale d’une expression texte selon les règles de la grammaire. Les différentes unités seront dotées d’une annotation permettant de savoir le rôle grammatical de chaque mot dans l’expression. Les annotations les plus courante sont (DT : Determiner, NN : noun , JJ : adjective,  RB: adverb, VB : verb,  PRP : Personal Pronoun…).\n",
    "\n",
    "NLTK offre une panoplie de taggers pour le pos-taggin qui recoivent une liste de tokens et leurs attribuent automatiquement  des tags en se basant sur des corpus d'apprentisgae.  \n",
    "\n",
    "Par defaut la methode *pos_tag* offre un pos_tagging standard (Recommandé) pour l'anglais et cela en se bsant sur le tagset *\"Penn Treebank\"*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLV-XP6_0yYu",
    "outputId": "2966704d-7310-4dfe-d2c5-5580e8c6cd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('i', 'NN'), ('am', 'VBP'), ('very', 'RB'), ('happy', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PRP'), ('.', '.'), ('I', 'PRP'), ('created', 'VBD'), ('this', 'DT'), ('course', 'NN'), ('for', 'IN'), ('you', 'PRP'), ('.', '.'), ('Good', 'JJ'), ('by', 'IN'), ('!', '.'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
    "words=word_tokenize(data)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyD2EoJB4kEJ"
   },
   "source": [
    "Dans le cas d'un document qui se compose de plusieurs phrases, il sera preferable d'utliser pos_tag_sents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTdsprzi0yYu",
    "outputId": "e82de46b-97bc-429e-d49e-00a5b47d7415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Hello', 'NNP'), (',', ','), ('i', 'NN'), ('am', 'VBP'), ('very', 'RB'), ('happy', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PRP'), ('.', '.')], [('I', 'PRP'), ('created', 'VBD'), ('this', 'DT'), ('course', 'NN'), ('for', 'IN'), ('you', 'PRP'), ('.', '.')], [('Good', 'JJ'), ('by', 'IN'), ('!', '.'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
    "\n",
    "\n",
    "sentences=sent_tokenize(data)\n",
    "\n",
    "list=[]\n",
    "for sentence in sentences:\n",
    "    list.append(word_tokenize(sentence))\n",
    "\n",
    "print(nltk.pos_tag_sents(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UISMSYrw4sHt"
   },
   "source": [
    "UnigramTagger permet d'attribuer aux mots leurs tags les plus frequents par rapport à un corpus d'apprentissage.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UP_6-5wg0yYu",
    "outputId": "8e182e72-a07e-4f28-9d66-f7e267143148"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dscon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121200039868434\n",
      "[('Hello', None), (',', ','), ('i', None), ('am', 'BEM'), ('very', 'QL'), ('happy', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PPSS'), ('.', '.'), ('I', 'PPSS'), ('created', 'VBN'), ('this', 'DT'), ('course', 'NN'), ('for', 'IN'), ('you', 'PPSS'), ('.', '.'), ('Good', 'JJ-TL'), ('by', 'IN'), ('!', '.'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\829503320.py:12: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(unigram_tagger.evaluate(test_sents))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n",
    "\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!. \"\n",
    "print(unigram_tagger.tag(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sSLwHFo0yYu"
   },
   "source": [
    "le modèle n-gram est une generalisation de l'unigram qui cnsidère également le contexte où apparait le mot en considerant les tags des n-1 mots precedents.\n",
    "\n",
    "bigram tagger est un exemple generateur pos-tagging n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqVvBkCw0yYu",
    "outputId": "00e31091-20fd-4c5e-a4b8-3f745e7cf1f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\88343320.py:9: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(bigram_tagger.evaluate(test_sents))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3515747783994468\n",
      "[('Hello', 'UH'), (',', ','), ('i', None), ('am', None), ('very', None), ('happy', None), ('to', None), ('meet', None), ('you', None), ('.', None), ('I', None), ('created', None), ('this', None), ('course', None), ('for', None), ('you', None), ('.', None), ('Good', None), ('by', None), ('!', None)]\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents()\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "\n",
    "print(bigram_tagger.evaluate(test_sents))\n",
    "\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "print(bigram_tagger.tag(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LYzZb5K0yYu"
   },
   "source": [
    "On peut combiner plusieurs taggers en les executant d'une maniere sequentielle comme montré dans l'exemple c-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQTiOflB0yYv",
    "outputId": "fde72f2c-c381-443f-aada-24c38415476c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452108043456593\n",
      "[('Hello', 'NN'), (',', ','), ('i', 'NN'), ('am', 'BEM'), ('very', 'QL'), ('happy', 'JJ'), ('to', 'TO'), ('meet', 'VB'), ('you', 'PPO'), ('.', '.'), ('I', 'PPSS'), ('created', 'VBN'), ('this', 'DT'), ('course', 'NN'), ('for', 'IN'), ('you', 'PPO'), ('.', '.'), ('Good', 'JJ-TL'), ('by', 'IN'), ('!', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\1528170503.py:10: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(t2.evaluate(test_sents))\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print(t2.evaluate(test_sents))\n",
    "\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "print(t2.tag(word_tokenize(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX_RmuuZ0yYv"
   },
   "source": [
    "Pour le moment la package nltk ne permet de faire le pos-tagging que pour l’anglais et le russe à l’aide du modèle « averaged_perceptron_tagger ».\n",
    "StanfordPOSTagguer permet faire du pos-tagging pour d’autre langues comme le français et l’arabe. Il suffit de télécharger les differents librairies nécessaires (https://nlp.stanford.edu/software/tagger.shtml) et utiliser celles qui correspondent à la langue comme présenté dans l’exemple ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9HoPuMa7FHK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# !unzip stanford-tagger-4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsfQiRCy0yYv",
    "outputId": "8de278d8-462f-4e6b-ab00-6e80ba036ff5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "root=\"stanford-postagger-full-2020-11-17\"\n",
    "stf = StanfordPOSTagger(root+'/models/french-ud.tagger',root+\"/stanford-postagger.jar\",encoding='utf8')\n",
    "tokens = nltk.word_tokenize(data)\n",
    "print(stf.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42NQzHRr0yYv"
   },
   "source": [
    "#8.\tAnalyse Sémantique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmxWc7_E0yYv"
   },
   "source": [
    "Un mot peut avoir plusieurs significations selon son contexte (les mots voisins et le rôle grammaticale). Par exemple, le mot anglais « break » possède 75 sens. Chose qui montre l’importance de la désambiguïsation lors de l’analyse d’un texte. Ci-dessous un extrait de la récupération des différents sens du mot « break » avec leurs annotations grammaticales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mzNoBhDq0yYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> some abrupt occurrence that interrupts an ongoing activity\n",
      ">>> an unexpected piece of good luck\n",
      ">>> (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n",
      ">>> a personal or social separation (as between opposing factions)\n",
      ">>> a pause from doing something (as work)\n",
      ">>> the act of breaking something\n",
      ">>> a time interval during which there is a temporary cessation of something\n",
      ">>> breaking of hard tissue such as bone\n",
      ">>> the occurrence of breaking\n",
      ">>> an abrupt change in the tone or register of the voice (as at puberty or due to emotion)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for synset in wordnet.synsets('break')[:10]:\n",
    "    print(\">>>\",synset.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCHTAEpW0yYv"
   },
   "source": [
    "Pour un synset bien determiné on peut recuperer la liste des termes qui partage le même sens (La même description du sens):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KCl3w4zU0yYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('interruption.n.02.interruption'), Lemma('interruption.n.02.break')]\n",
      "[Lemma('break.n.02.break'), Lemma('break.n.02.good_luck'), Lemma('break.n.02.happy_chance')]\n",
      "[Lemma('fault.n.04.fault'), Lemma('fault.n.04.faulting'), Lemma('fault.n.04.geological_fault'), Lemma('fault.n.04.shift'), Lemma('fault.n.04.fracture'), Lemma('fault.n.04.break')]\n",
      "[Lemma('rupture.n.02.rupture'), Lemma('rupture.n.02.breach'), Lemma('rupture.n.02.break'), Lemma('rupture.n.02.severance'), Lemma('rupture.n.02.rift'), Lemma('rupture.n.02.falling_out')]\n",
      "[Lemma('respite.n.02.respite'), Lemma('respite.n.02.recess'), Lemma('respite.n.02.break'), Lemma('respite.n.02.time_out')]\n",
      "[Lemma('breakage.n.03.breakage'), Lemma('breakage.n.03.break'), Lemma('breakage.n.03.breaking')]\n",
      "[Lemma('pause.n.01.pause'), Lemma('pause.n.01.intermission'), Lemma('pause.n.01.break'), Lemma('pause.n.01.interruption'), Lemma('pause.n.01.suspension')]\n",
      "[Lemma('fracture.n.01.fracture'), Lemma('fracture.n.01.break')]\n",
      "[Lemma('break.n.09.break')]\n",
      "[Lemma('break.n.10.break')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "seynsets= wordnet.synsets('break')\n",
    "\n",
    "for synset in seynsets[:10]:\n",
    "    print(synset.lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXk0iyGe0yYw"
   },
   "source": [
    "On peut même récuperer le terme correspedant dans une autre langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wS38aezy0yYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('interruption.n.02.interruption')]\n",
      "[Lemma('break.n.02.casser')]\n",
      "[Lemma('fault.n.04.casser'), Lemma('fault.n.04.cassure'), Lemma('fault.n.04.fracture')]\n",
      "[Lemma('rupture.n.02.briser'), Lemma('rupture.n.02.chute'), Lemma('rupture.n.02.rift'), Lemma('rupture.n.02.rompre')]\n",
      "[Lemma('respite.n.02.casser'), Lemma('respite.n.02.pause'), Lemma('respite.n.02.repos'), Lemma('respite.n.02.trêve')]\n",
      "[Lemma('breakage.n.03.cassure')]\n",
      "[Lemma('pause.n.01.intermission'), Lemma('pause.n.01.interruption'), Lemma('pause.n.01.pause'), Lemma('pause.n.01.repos'), Lemma('pause.n.01.trêve')]\n",
      "[Lemma('fracture.n.01.casser'), Lemma('fracture.n.01.fracture')]\n",
      "[Lemma('break.n.09.casser')]\n",
      "[Lemma('break.n.10.casser')]\n",
      "[Lemma('interruption.n.02.توقُّف')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[Lemma('breakage.n.03.تحْطِيم'), Lemma('breakage.n.03.تكْسِير'), Lemma('breakage.n.03.كسْر')]\n",
      "[Lemma('pause.n.01.إِرْجاء'), Lemma('pause.n.01.اِسْتِراحة'), Lemma('pause.n.01.اِيقاف'), Lemma('pause.n.01.تعْلِيق'), Lemma('pause.n.01.توقُّف')]\n",
      "[Lemma('fracture.n.01.كسْر')]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "seynsets= wordnet.synsets('break')\n",
    "\n",
    "#Francais\n",
    "for synset in seynsets[:10]:\n",
    "    print(synset.lemmas(lang='fra'))\n",
    "\n",
    "#Arabe\n",
    "for synset in seynsets[:10]:\n",
    "    print(synset.lemmas(lang='arb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28aLdZ7o0yYw"
   },
   "source": [
    "pour la liste des langues dsiponibles executer: sorted(wn.langs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLBiK1XA0yYw"
   },
   "source": [
    "La bibliothèque nltk offre à travers le module **wsd** la possibilité de détecter le sens d’un mot en fonction de son contexte. A cette fin, l’algorithme **Lesk** est utilisé pour réaliser une désambiguïsation du sens d’un mot en retournant le sens qui a permis d’avoir le plus grand nombre de termes en intersection avec le contexte du mot pour lequel on est en train de chercher le sens exact. L’algorithme ne retourne aucun sens s’il n’arrive pas à réaliser la désambiguïsation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LIRvjkce0yYx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "context= word_tokenize(\"I've just finished the first step of the competition. I need a little break to catch my breath\")\n",
    "synset=lesk(context, 'break','n')\n",
    "print(synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bud4f9M-0yYx"
   },
   "source": [
    "L’exemple ci-dessus montre que l’algorithme n’est pas assez performant. D’autres algorithmes peuvent être utilisés en se basant sur les bibliothèques baseline, pywsd ou spaCy. Ci-dessous un autre exemple avec la bibliotheque pywsd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IrQi6KqJ0yYx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rupture.n.02')\n",
      "a personal or social separation (as between opposing factions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "took 3.2237613201141357 secs.\n"
     ]
    }
   ],
   "source": [
    "# !pip install pywsd\n",
    "# pip install -U pywsd\n",
    "#pip install wn==0.0.22\n",
    "from pywsd.lesk import simple_lesk\n",
    "sent = \"I've just finished the first step of the competition. I need a little break to catch my breath\"\n",
    "ambiguous = 'break'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print (answer)\n",
    "print (answer.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpU8Bt8E0yYx"
   },
   "source": [
    "#9.\tAnalyse syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if-fxqRf0yYx"
   },
   "source": [
    "L'objectif de cette section est d'analyser la structure grammaticale des phrases au lieu de se focaliser d'une manière individuelle sur les mots les composant. Nous nous contenetant dans un premier temps de l'approche grammaticale pour realiser l'inference de la structure arborescente d'une phrase.\n",
    "\n",
    "Il existe plusieurs bibliotehques permettant de reéaliser cette tâche. ci-dessous deux exemples avec les bibliotheque stanford et spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7m4o5uY0yYx"
   },
   "source": [
    "### Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er2VWp3E0yYy"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "from nltk.parse import stanford\n",
    "!os.environ['STANFORD_PARSER'] = 'stanford-parser-full-2020-11-17'\n",
    "!os.environ['STANFORD_MODELS'] = 'stanford-parser-full-2020-11-17'\n",
    "\n",
    "parser = stanford.StanfordParser(model_path=\"stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "sentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\n",
    "\n",
    "\n",
    "#Formatted\n",
    "\n",
    "for line in sentences:\n",
    "    for sentence in line:\n",
    "        print(sentence)\n",
    "\n",
    "\n",
    "'''\n",
    "# GUI\n",
    "for line in sentences:\n",
    "    for sentence in line:\n",
    "        sentence.draw()\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjDbbhkW0yYy"
   },
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRMSot9c0yYy"
   },
   "source": [
    "spaCy une bibliothèque de la NLP qui est très puissante (elle est orientée production, non uniquement pour la recherche ou l’apprentissage de la NLP), totalement écrite python, gratuite et libre.\n",
    "Par défaut, lorsqu’on fait appel au module **nlp** de spaCy, les opérations suivantes sont exécutées : Segmentation, pos-tagging, analyse syntaxique, NER (Named Entity Recognition), etc.  Un objet Doc est retourné à l’issue de toutes les opérations et qui encapsule tous les resultats de l’analyse.\n",
    "\n",
    "L’exemple ci-dessous montre comment extraire les différentes informations à partir d’un objet Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aisuYqTT0yYy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/token: Le /lemma: le Xx True True /POS: DET /PARS: big det /NER: O \n",
      "/token: big /lemma: big xxx True False /POS: PRON /PARS: data nsubj /NER: O \n",
      "/token: data /lemma: dater xxxx True False /POS: VERB /PARS: data ROOT /NER: O \n",
      "/token:   /lemma:     False False /POS: SPACE /PARS: data dep /NER: O \n",
      "/token: « /lemma: « « False False /POS: NUM /PARS: data xcomp /NER: O \n",
      "/token: grosses /lemma: grosse xxxx True False /POS: NOUN /PARS: grosses ROOT /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: grosses amod /NER: O \n",
      "/token: » /lemma: » » False False /POS: PUNCT /PARS: grosses punct /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: anglais case /NER: O \n",
      "/token: anglais /lemma: anglais xxxx True False /POS: NOUN /PARS: grosses nmod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: mégadonnées det /NER: O \n",
      "/token: mégadonnées /lemma: mégadonnée xxxx True False /POS: NOUN /PARS: désigne nsubj /NER: O \n",
      "/token: ou /lemma: ou xx True True /POS: CCONJ /PARS: données cc /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: données det /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: mégadonnées conj /NER: O \n",
      "/token: massives /lemma: massif xxxx True False /POS: ADJ /PARS: données amod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: désigne /lemma: désigner xxxx True False /POS: VERB /PARS: désigne ROOT /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: ressources det /NER: O \n",
      "/token: ressources /lemma: ressource xxxx True False /POS: NOUN /PARS: désigne obj /NER: O \n",
      "/token: d’ /lemma: d’ x’ False True /POS: ADJ /PARS: désigne dep /NER: O \n",
      "/token: informations /lemma: information xxxx True False /POS: NOUN /PARS: désigne obj /NER: O \n",
      "/token: dont /lemma: dont xxxx True True /POS: PRON /PARS: caractéristiques nmod /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: caractéristiques det /NER: O \n",
      "/token: caractéristiques /lemma: caractéristique xxxx True False /POS: NOUN /PARS: imposent nsubj /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: termes case /NER: O \n",
      "/token: termes /lemma: terme xxxx True False /POS: NOUN /PARS: caractéristiques nmod /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: volume case /NER: O \n",
      "/token: volume /lemma: volume xxxx True False /POS: NOUN /PARS: termes nmod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: vélocité punct /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: vélocité case /NER: O \n",
      "/token: vélocité /lemma: vélocité xxxx True False /POS: NOUN /PARS: termes conj /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: variété cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: variété case /NER: O \n",
      "/token: variété /lemma: variété xxxx True False /POS: AUX /PARS: termes conj /NER: O \n",
      "/token: imposent /lemma: imposer xxxx True False /POS: VERB /PARS: informations acl:relcl /NER: O \n",
      "/token: l’ /lemma: l’ x’ False True /POS: NOUN /PARS: utilisation det /NER: O \n",
      "/token: utilisation /lemma: utilisation xxxx True False /POS: NOUN /PARS: imposent obj /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: technologies case /NER: O \n",
      "/token: technologies /lemma: technologie xxxx True False /POS: NOUN /PARS: utilisation nmod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: méthodes cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: méthodes case /NER: O \n",
      "/token: méthodes /lemma: méthode xxxx True False /POS: NOUN /PARS: technologies conj /NER: O \n",
      "/token: analytiques /lemma: analytique xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: particulières /lemma: particulier xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: pour /lemma: pour xxxx True True /POS: ADP /PARS: générer mark /NER: O \n",
      "/token: générer /lemma: générer xxxx True False /POS: VERB /PARS: imposent advcl /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: valeur case /NER: O \n",
      "/token: la /lemma: le xx True True /POS: DET /PARS: valeur det /NER: O \n",
      "/token: valeur /lemma: valeur xxxx True False /POS: NOUN /PARS: générer obl:arg /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: imposent punct /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: dépassent cc /NER: O \n",
      "/token: qui /lemma: qui xxx True True /POS: PRON /PARS: dépassent nsubj /NER: O \n",
      "/token: dépassent /lemma: dépasser xxxx True False /POS: VERB /PARS: imposent conj /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: général case /NER: O \n",
      "/token: général /lemma: général xxxx True False /POS: NOUN /PARS: dépassent obl:mod /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: capacités det /NER: O \n",
      "/token: capacités /lemma: capacité xxxx True False /POS: NOUN /PARS: dépassent obj /NER: O \n",
      "/token: d' /lemma: de x' False True /POS: ADP /PARS: seule case /NER: O \n",
      "/token: une /lemma: un xxx True True /POS: DET /PARS: seule det /NER: O \n",
      "/token: seule /lemma: seul xxxx True True /POS: ADJ /PARS: capacités amod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: machine cc /NER: O \n",
      "/token: unique /lemma: unique xxxx True False /POS: ADJ /PARS: machine amod /NER: O \n",
      "/token: machine /lemma: machine xxxx True False /POS: NOUN /PARS: seule conj /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: nécessitent cc /NER: O \n",
      "/token: nécessitent /lemma: nécessiter xxxx True False /POS: VERB /PARS: dépassent conj /NER: O \n",
      "/token: des /lemma: un xxx True True /POS: DET /PARS: traitements det /NER: O \n",
      "/token: traitements /lemma: traitement xxxx True False /POS: NOUN /PARS: nécessitent obj /NER: O \n",
      "/token: parallélisés /lemma: paralléliser xxxx True False /POS: VERB /PARS: traitements acl /NER: O \n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download fr_core_news_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"/token:\",token.text, \"/lemma:\",token.lemma_, token.shape_, token.is_alpha, token.is_stop,\"/POS:\", token.tag_, \"/PARS:\", token.head, token.dep_, \"/NER:\", token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXv100B-0yYy"
   },
   "source": [
    "Pour récuperer L'arbre de dependance syntaxique evec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xFFj4X6p0yYy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           | Relation | Head            |POS        | Children            \n",
      "----------------------------------------------------------------------\n",
      "Hello           | intj     | is              |AUX        | []                  \n",
      ",               | punct    | is              |AUX        | []                  \n",
      "My              | poss     | name            |NOUN       | []                  \n",
      "name            | nsubj    | is              |AUX        | [My]                \n",
      "is              | ROOT     | is              |AUX        | [Hello, ,, name, Melroy, .]\n",
      "Melroy          | attr     | is              |AUX        | []                  \n",
      ".               | punct    | is              |AUX        | []                  \n",
      "What            | attr     | is              |AUX        | []                  \n",
      "is              | ROOT     | is              |AUX        | [What, name, ?]     \n",
      "your            | poss     | name            |NOUN       | []                  \n",
      "name            | nsubj    | is              |AUX        | [your]              \n",
      "?               | punct    | is              |AUX        | []                  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dd8145be57a447b39d6bdc698a1edd22-0\" class=\"displacy\" width=\"1130\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hello,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">My</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">name</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">Melroy.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">What</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">your</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">name?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,2.0 410.0,2.0 410.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,124.0 L182,112.0 198,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-2\" stroke-width=\"2px\" d=\"M310,122.0 C310,62.0 405.0,62.0 405.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,124.0 L302,112.0 318,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-3\" stroke-width=\"2px\" d=\"M430,122.0 C430,62.0 525.0,62.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,124.0 L533.0,112.0 517.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-4\" stroke-width=\"2px\" d=\"M670,122.0 C670,62.0 765.0,62.0 765.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,124.0 L662,112.0 678,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-5\" stroke-width=\"2px\" d=\"M910,122.0 C910,62.0 1005.0,62.0 1005.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910,124.0 L902,112.0 918,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd8145be57a447b39d6bdc698a1edd22-0-6\" stroke-width=\"2px\" d=\"M790,122.0 C790,2.0 1010.0,2.0 1010.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd8145be57a447b39d6bdc698a1edd22-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1010.0,124.0 L1018.0,112.0 1002.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, My name is Melroy. What is your name?\")\n",
    "\n",
    "#Visualisation 1\n",
    "print (\"{:<15} | {:<8} | {:<15} |{:<10} | {:<20}\".format('Token','Relation','Head','POS', 'Children'))\n",
    "print (\"-\" * 70)\n",
    "for token in doc:\n",
    "  # Print the token, dependency nature, head and all dependents of the token\n",
    "  print (\"{:<15} | {:<8} | {:<15} |{:<10} | {:<20}\"\n",
    "         .format(str(token.text), str(token.dep_), str(token.head.text), str(token.head.pos_), str([child for child in token.children])))\n",
    "\n",
    "\n",
    "#Visualisation 2 (graphique)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AONamkg10yYy"
   },
   "source": [
    "# 10.\tExercices :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4czGi0KMX3MX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnE8UK8y0yYy"
   },
   "source": [
    "## 10.1 Exercice 1: Traduction automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O66-7G5p0yYz"
   },
   "source": [
    "On desire assister l'utilisateur pendant la traduction de l'anglais vers le francais.\n",
    "\n",
    "* Constituer le contexte du document en recuperant tous les termes sigfificatifs\n",
    "* Découper le texte en des phrases simples et recuperer les tags de leurs mots.\n",
    "* Pour chaque phrase récuperer le sens exacte de chaque terme en se basant sur leurs Tags et leur contexts\n",
    "* Récuperer les termes correspendant en francais\n",
    "* Pour chaque phrase afficher à l'utilisateur les propostions de traduction pour les nom, les adjectifs et les verbes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition - Exercice 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant Terms from the Document (Context): ['Inheritance', 'basic', 'concept', 'Object', 'Oriented', 'Programming', 'allows', 'class', 'use', 'properties', 'methods', 'class']\n",
      "\n",
      "Tagged Sentences:\n",
      "[('Inheritance', 'NOUN'), ('is', 'AUX'), ('a', 'DET'), ('basic', 'ADJ'), ('concept', 'NOUN'), ('of', 'ADP'), ('Object', 'NOUN'), ('-', 'PUNCT'), ('Oriented', 'VERB'), ('Programming', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('It', 'PRON'), ('allows', 'VERB'), ('a', 'DET'), ('class', 'NOUN'), ('to', 'PART'), ('use', 'VERB'), ('properties', 'NOUN'), ('and', 'CCONJ'), ('methods', 'NOUN'), ('of', 'ADP'), ('another', 'DET'), ('class', 'NOUN'), ('.', 'PUNCT')]\n",
      "\n",
      "Translation Suggestions (For Nouns, Adjectives, and Verbs):\n",
      "Original: Inheritance (NOUN) -> Suggested Translation: héritage\n",
      "Original: basic (ADJ) -> Suggested Translation: basique\n",
      "Original: concept (NOUN) -> Suggested Translation: concept\n",
      "Original: Object (NOUN) -> Suggested Translation: chose\n",
      "Original: Oriented (VERB) -> Suggested Translation: adapter\n",
      "Original: allows (VERB) -> Suggested Translation: autoriser\n",
      "Original: class (NOUN) -> Suggested Translation: classe\n",
      "Original: use (VERB) -> Suggested Translation: de\n",
      "Original: properties (NOUN) -> Suggested Translation: accessoire\n",
      "Original: methods (NOUN) -> Suggested Translation: methods\n",
      "Original: class (NOUN) -> Suggested Translation: classe\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"Inheritance is a basic concept of Object-Oriented Programming. It allows a class to use properties and methods of another class.\"\"\"\n",
    "\n",
    "def extract_significant_terms(doc):\n",
    "    \"\"\"\n",
    "    Extract all significant terms from the document.\n",
    "    \"\"\"\n",
    "    significant_terms = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return significant_terms\n",
    "\n",
    "def split_and_tag_sentences(doc):\n",
    "    \"\"\"\n",
    "    Split the text into simple sentences and retrieve the POS tags for each word.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        tagged_words = [(token.text, token.pos_) for token in sent]\n",
    "        sentences.append(tagged_words)\n",
    "    return sentences\n",
    "\n",
    "def retrieve_exact_meaning_synset(word, sentence):\n",
    "    \"\"\"\n",
    "    Use WordNet to retrieve the exact sense (meaning) of the word in the context of the sentence.\n",
    "    \"\"\"\n",
    "    synset = lesk(sentence, word)\n",
    "    if synset:\n",
    "        return synset\n",
    "    return None\n",
    "\n",
    "def translate_with_synset(word, sentence):\n",
    "    \"\"\"\n",
    "    Retrieve French translation of the word using its synset.\n",
    "    \"\"\"\n",
    "    synset = retrieve_exact_meaning_synset(word, sentence)\n",
    "    if synset:\n",
    "        translations = synset.lemmas('fra')\n",
    "        if translations:\n",
    "            return translations[0].name()\n",
    "    return word\n",
    "\n",
    "def display_translation_suggestions(tagged_sentence, sentence):\n",
    "    \"\"\"\n",
    "    Display translation suggestions for nouns, adjectives, and verbs.\n",
    "    \"\"\"\n",
    "    suggestions = []\n",
    "    for word, pos in tagged_sentence:\n",
    "        if pos in [\"NOUN\", \"VERB\", \"ADJ\"]:\n",
    "            french_term = translate_with_synset(word, sentence)\n",
    "            suggestions.append((word, french_term, pos))\n",
    "    return suggestions\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "significant_terms = extract_significant_terms(doc)\n",
    "print(\"Significant Terms from the Document (Context):\", significant_terms)\n",
    "\n",
    "tagged_sentences = split_and_tag_sentences(doc)\n",
    "print(\"\\nTagged Sentences:\")\n",
    "for sentence in tagged_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nTranslation Suggestions (For Nouns, Adjectives, and Verbs):\")\n",
    "for sentence in doc.sents:\n",
    "    tagged_sentence = [(token.text, token.pos_) for token in sentence]\n",
    "    suggestions = display_translation_suggestions(tagged_sentence, sentence)\n",
    "    for original, french, pos in suggestions:\n",
    "        print(f\"Original: {original} ({pos}) -> Suggested Translation: {french}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDlaHpvh0yYz"
   },
   "source": [
    "## 10.2 Exercice 2: Detection du plagiarisme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3kofjN80yYz"
   },
   "source": [
    "L’objectif de cet exercice est de détecter le pélagianisme à partir de wikipedia pendant la préparation des réponses à un certain nombre de questions sur des connaissances en informatique. Le dataset utilisé peut-être récupéré à partir du lien suivant :Cliquer <a href=\"https://drive.google.com/file/d/11y6oqH4kjlIBTQj3qDODi7bIfp9g2Rks/view?usp=drive_link\" target=\"_blank\">ICI</a>\n",
    "\n",
    "Pour ce faire, nous nous basant sur le calcul des similarités entre les réponses des candidats et les définitions exactes trouvées sur Wikipédia. Deux méthodes de calcul de similarité sont à utiliser, à savoir, la similarité syntaxique (orientée caractères) et la similarité sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDhZb10uQL6O"
   },
   "source": [
    "### Exploration du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iQQijONQZFf"
   },
   "source": [
    "Le corpus utilisé dans cette exercice était realisé dans le cadre d'un travail de recherche \"Clough, P., Stevenson, M. Developing a corpus of plagiarised short answers. Lang Resources & Evaluation 45, 5–24 (2011). https://doi.org/10.1007/s10579-009-9112-1\"\n",
    "\n",
    "Le corpus se compose de plusieurs fichiers texte dont les carateristiques sont decrites dans corpus-final09.xls.\n",
    "\n",
    "Chaque fichier est associé à une tache (tasks a-e) et à un type de plagiarisme:\n",
    "\n",
    "*   cut: du copier coller à partir du texte original.\n",
    "*   light: quelquie extrait du texte original avec queklques reformulations.\n",
    "*   heavy: du copier colller vaec reformulation.\n",
    "*   non: pas de copier coller.\n",
    "*   orig: texte origional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkLMpiqv0yYz"
   },
   "source": [
    "### Similarité Syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq0gFIsO0yYz"
   },
   "source": [
    "Pour la similarité  syntaxique entre des documents courts (des phrases) on peut recrorir à l'utilisation de l'un des algorithmes suivants:\n",
    "* Longest Common Sequence (LCS)\n",
    "* Set features\n",
    "* Word Order Similarity\n",
    "* n-gram sentences\n",
    "* Jaro-Winkler\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBrn1Jo00yYz"
   },
   "source": [
    "* Recuperer le dataset du plagiarisme?\n",
    "* Réaliser les différentes tâches de prétraitement?\n",
    "* Calculer les similarités syntaxiques entre les réponses des étudiants et les reponses originales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKctuAVhkBCB",
    "outputId": "eca94169-d4ee-4506-8f3a-a39cf3152639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glob2 in c:\\users\\dscon\\anaconda3\\lib\\site-packages (0.7)\n"
     ]
    }
   ],
   "source": [
    "#glob lib\n",
    "!pip install glob2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgTyUYL00yYz"
   },
   "source": [
    "### Similarité Sémantique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQZB-guC0yYz"
   },
   "source": [
    "WordNet est une base de données lexicale qui comporte des concepts (termes) classifiés et reliés les uns aux autres à travers des realtions semantiques\n",
    "\n",
    "La composante principale de wordNet est le synset (synonym set) tel que chacun contient plusieurs mots qui partagent le même sens (des lemmas). Egalement, un mot peut appartenir à plusieurs synsets à la foix.\n",
    "\n",
    "l'exemple suivant montre comment recuperer les synsets d'un mot et comment recuperer ses synonymes pour un sens particuliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FMeuLT1K0yY0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer sens in wordNet:\n",
      " \t Sens : 0\n",
      " \t\t Sens definition: a machine for performing calculations automatically\n",
      "\t\t Lemmas for sense :['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      " \t Sens : 1\n",
      " \t\t Sens definition: an expert at calculation (or at operating calculating machines)\n",
      "\t\t Lemmas for sense :['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "computer_synsets = wn.synsets(\"computer\")\n",
    "print(\"Computer sens in wordNet:\")\n",
    "i=0\n",
    "for sense in computer_synsets:\n",
    "    print(\" \\t Sens :\", i)\n",
    "    print(\" \\t\\t Sens definition: \"+sense.definition())\n",
    "    lemmas = [l.name() for l in sense.lemmas()]\n",
    "    print(\"\\t\\t Lemmas for sense :\" +str(lemmas))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJQURAq40yY0"
   },
   "source": [
    "Pour la similarité sémantique, la bibliothèque nltk et à travers le module wordnet permet de mesurer la distance ou la similarité sémantique entre les sens des mots. Ainsi, en récupérant les sens synset1 et synset2 de deux mots quelconques plusieurs façons sont possibles pour calculer leur similarité:\n",
    "\n",
    "* synset1.path_similarity(synset2) : retourne leur ordre de similarité sous forme d’une valeur numérique entre 0 et 1 en se basant sur le plus court chemin qui relie les deux sens dans l’arborescence de wordnet.\n",
    "* synset1.lch_similarity(synset2): qui se base sur l’algorithme Leacock-Chodorow\n",
    "* Synset1.wup_similarity(synset2): qui se base sur l’algorithme Wu-Palmer\n",
    "* synset1.res_similarity(synset2, ic): qui se base sur l’algorithme Resnik:\n",
    "* synset1.jcn_similarity(synset2, ic): qui se base sur l’algorithme Jiang-Conrath\n",
    "* synset1.lin_similarity(synset2, ic): qui se base sur l’algorithme Lin\n",
    "\n",
    "l'exemple suivant montre comment calculer les similarité entres les sens des termes computer et device en se basant sur les metriques Leacock-Chodorow et Wu-Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "PQnsr0270yY0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lch</th>\n",
       "      <td>2.538974</td>\n",
       "      <td>1.072637</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.558145</td>\n",
       "      <td>1.440362</td>\n",
       "      <td>1.440362</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>1.239691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wup</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "lch  2.538974  1.072637  0.693147  1.558145  1.440362  1.440362  1.335001   \n",
       "wup  0.875000  0.142857  0.100000  0.588235  0.555556  0.500000  0.181818   \n",
       "\n",
       "            7         8         9  \n",
       "lch  0.864997  1.335001  1.239691  \n",
       "wup  0.117647  0.470588  0.444444  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "computer_synsets = wn.synsets(\"computer\")\n",
    "device_synsets = wn.synsets(\"device\")\n",
    "lch=[]\n",
    "wup=[]\n",
    "\n",
    "\n",
    "for s1 in computer_synsets:\n",
    "    for s2 in device_synsets:\n",
    "        lch.append(s1.lch_similarity(s2))\n",
    "        wup.append(s1.wup_similarity(s2))\n",
    "\n",
    "pd.DataFrame([lch,wup],[\"lch\",\"wup\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSW-7rBk0yY0"
   },
   "source": [
    "Souvent on aura besoin de recuperer les sens exactes des termes dans leurs contextes afin mesurer leurs similarité d'une manière plus precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Q8T7BV-k0yY0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a machine for performing calculations automatically\n",
      "a machine for performing calculations automatically\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def WSD(word, doc):\n",
    "    context= word_tokenize(doc)\n",
    "    sens=lesk(context, word)\n",
    "    return sens\n",
    "\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "\n",
    "\n",
    "print(WSD(\"Computer\", doc1).definition())\n",
    "print(WSD(\"Computer\", doc2).definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfYvxSDE0yY0"
   },
   "source": [
    "Pour calculer la distance semantique entre deux documents, on aura besoin de calculer les similarités semantiques entre leurs mots deux à deux ou utiliser par example la distance de Hausdorff ou l'indic de Jaccard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pXtXU360yY0"
   },
   "source": [
    "* Defnir la fonction SemanticDistanceDocs(doc1,doc2) qui permet de calculer la distance semantique totale entre deux documents texte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "cIciQT0Y0yY0"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "\n",
    "def WSD(word, sentence):\n",
    "    return lesk(sentence, word) \n",
    "\n",
    "def synset_similarity(synset1, synset2):\n",
    "    if synset1 is None or synset2 is None:\n",
    "        return 0  # Aucun synset trouvé pour l'un des mots, pas de similarité possible\n",
    "    return synset1.wup_similarity(synset2) or 0  # Wu-Palmer Similarity\n",
    "\n",
    "\n",
    "def SemanticDistanceDocs(doc1, doc2):\n",
    "    # Tokenisation des documents\n",
    "    words_doc1 = word_tokenize(doc1)\n",
    "    words_doc2 = word_tokenize(doc2)\n",
    "    \n",
    "    # Désambiguïsation des sens pour chaque mot dans les deux documents\n",
    "    senses_doc1 = [WSD(word, doc1) for word in words_doc1]\n",
    "    senses_doc2 = [WSD(word, doc2) for word in words_doc2]\n",
    "    \n",
    "    # Calculer la similarité entre tous les sens (synsets) des deux documents\n",
    "    total_similarity = 0\n",
    "    count = 0\n",
    "    for sense1 in senses_doc1:\n",
    "        for sense2 in senses_doc2:\n",
    "            similarity = synset_similarity(sense1, sense2)\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    " \n",
    "\n",
    "    # Si aucun synset n'a pu être trouvé, retourner 0\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    # Retourner la similarité moyenne entre les synsets des deux documents\n",
    "    average_similarity = total_similarity / count\n",
    "    return 1 - average_similarity  # La distance est l'inverse de la similarité\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111448876638121"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SemanticDistanceDocs(doc1, doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n8GPch00yY1"
   },
   "source": [
    "* Calculer les similarités sémantiqueq entre les réponses des étudiants et les définitions trouvées sur wikipedia? considerer par example que:\n",
    "  *      cut: correspond à une similarité entre 75% et 100%\n",
    "  *      heavy: correspond à une similarité entre 50% et 75%\n",
    "  *      light: correspond à une similarité entre 25% et 50%\n",
    "  *      Non: correspond à une similarité entre 0% et 25%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def NLP(text):\n",
    "    lemmes = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text) if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "    return lemmes\n",
    "\n",
    "for file in glob.glob(\"*user.txt\"):\n",
    "    with open(file, 'r') as f:\n",
    "        text=f.read()\n",
    "        task=file.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xlrd==2.0.1\n",
    "# !pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10.2 Exercice 2: Detection du plagiarisme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calcul de similarités Syntaxiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original\\\\orig_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original\\\\orig_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original\\\\orig_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original\\\\orig_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original\\\\orig_taske.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "path = 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Original/*.txt'\n",
    "def txts_file_to_df(path):\n",
    "    file_paths = glob.glob(path)\n",
    "    print(file_paths)\n",
    "    data = []\n",
    "    for filepath in file_paths:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
    "                content = file.read()\n",
    "        filename = os.path.basename(filepath)\n",
    "        data.append({\n",
    "            \"File\": filename,\n",
    "            'Content': content,\n",
    "            'Task': filename.split('task')[-1].split('.')[0]\n",
    "        })\n",
    "    df = pd.DataFrame(data)    \n",
    "    return df\n",
    "\n",
    "response_files = txts_file_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pA_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pA_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pA_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pA_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pA_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pB_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pB_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pB_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pB_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pB_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pC_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pC_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pC_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pC_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pC_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pD_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pD_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pD_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pD_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pD_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pE_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pE_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pE_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pE_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g0pE_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pA_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pA_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pA_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pA_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pA_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pB_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pB_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pB_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pB_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pB_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pD_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pD_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pD_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pD_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g1pD_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pA_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pA_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pA_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pA_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pA_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pB_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pB_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pB_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pB_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pB_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pC_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pC_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pC_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pC_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pC_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pE_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pE_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pE_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pE_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g2pE_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pA_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pA_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pA_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pA_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pA_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pB_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pB_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pB_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pB_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pB_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pC_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pC_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pC_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pC_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g3pC_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pB_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pB_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pB_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pB_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pB_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pC_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pC_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pC_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pC_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pC_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pD_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pD_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pD_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pD_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pD_taske.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pE_taska.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pE_taskb.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pE_taskc.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pE_taskd.txt', 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users\\\\g4pE_taske.txt']\n"
     ]
    }
   ],
   "source": [
    "path = 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/plagitat/Users/*.txt'\n",
    "users_files = txts_file_to_df(path)\n",
    "\n",
    "def getName(text):\n",
    "    return str(text).split('_')[0]\n",
    "\n",
    "users_files['User'] = users_files['File'].apply(getName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Content</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orig_taska.txt</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orig_taskb.txt</td>\n",
       "      <td>PageRank is a link analysis algorithm used by ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>orig_taskc.txt</td>\n",
       "      <td>Vector space model (or term vector model) is a...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orig_taskd.txt</td>\n",
       "      <td>In probability theory, Bayes' theorem (often c...</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orig_taske.txt</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File                                            Content Task\n",
       "0  orig_taska.txt  In object-oriented programming, inheritance is...    a\n",
       "1  orig_taskb.txt  PageRank is a link analysis algorithm used by ...    b\n",
       "2  orig_taskc.txt  Vector space model (or term vector model) is a...    c\n",
       "3  orig_taskd.txt  In probability theory, Bayes' theorem (often c...    d\n",
       "4  orig_taske.txt  In mathematics and computer science, dynamic p...    e"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column name content to Response\n",
    "response_files.rename(columns={'Content': 'OResponse'}, inplace=True)\n",
    "users_files.rename(columns={'Content': 'UResponse'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>UResponse</th>\n",
       "      <th>Task</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "      <td>a</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>PageRank is a link analysis algorithm used by ...</td>\n",
       "      <td>b</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>The vector space model (also called, term vect...</td>\n",
       "      <td>c</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>Bayes’ theorem was names after Rev Thomas Baye...</td>\n",
       "      <td>d</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>Dynamic Programming is an algorithm design tec...</td>\n",
       "      <td>e</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>g4pE_taska.txt</td>\n",
       "      <td>Object oriented programming is a  style of pro...</td>\n",
       "      <td>a</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>g4pE_taskb.txt</td>\n",
       "      <td>PageRankalgorithm is also known as link analys...</td>\n",
       "      <td>b</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>g4pE_taskc.txt</td>\n",
       "      <td>The definition of term depends on the applicat...</td>\n",
       "      <td>c</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>g4pE_taskd.txt</td>\n",
       "      <td>\"Bayes' Theorem\" or \"Bayes' Rule\", or somethin...</td>\n",
       "      <td>d</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>g4pE_taske.txt</td>\n",
       "      <td>Dynamic programming is a method for efficient...</td>\n",
       "      <td>e</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              File                                          UResponse Task  \\\n",
       "0   g0pA_taska.txt  Inheritance is a basic concept of Object-Orien...    a   \n",
       "1   g0pA_taskb.txt  PageRank is a link analysis algorithm used by ...    b   \n",
       "2   g0pA_taskc.txt  The vector space model (also called, term vect...    c   \n",
       "3   g0pA_taskd.txt  Bayes’ theorem was names after Rev Thomas Baye...    d   \n",
       "4   g0pA_taske.txt  Dynamic Programming is an algorithm design tec...    e   \n",
       "..             ...                                                ...  ...   \n",
       "90  g4pE_taska.txt  Object oriented programming is a  style of pro...    a   \n",
       "91  g4pE_taskb.txt  PageRankalgorithm is also known as link analys...    b   \n",
       "92  g4pE_taskc.txt  The definition of term depends on the applicat...    c   \n",
       "93  g4pE_taskd.txt  \"Bayes' Theorem\" or \"Bayes' Rule\", or somethin...    d   \n",
       "94  g4pE_taske.txt   Dynamic programming is a method for efficient...    e   \n",
       "\n",
       "    User  \n",
       "0   g0pA  \n",
       "1   g0pA  \n",
       "2   g0pA  \n",
       "3   g0pA  \n",
       "4   g0pA  \n",
       "..   ...  \n",
       "90  g4pE  \n",
       "91  g4pE  \n",
       "92  g4pE  \n",
       "93  g4pE  \n",
       "94  g4pE  \n",
       "\n",
       "[95 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Group</th>\n",
       "      <th>Person</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "      <th>Native English</th>\n",
       "      <th>Knowledge</th>\n",
       "      <th>Difficulty</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>a</td>\n",
       "      <td>non</td>\n",
       "      <td>native</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>b</td>\n",
       "      <td>cut</td>\n",
       "      <td>native</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>c</td>\n",
       "      <td>light</td>\n",
       "      <td>native</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>d</td>\n",
       "      <td>heavy</td>\n",
       "      <td>native</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>e</td>\n",
       "      <td>non</td>\n",
       "      <td>native</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>g0pA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>g4pE_taska.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>a</td>\n",
       "      <td>heavy</td>\n",
       "      <td>non-native</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>g4pE_taskb.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>b</td>\n",
       "      <td>light</td>\n",
       "      <td>non-native</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>g4pE_taskc.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>c</td>\n",
       "      <td>cut</td>\n",
       "      <td>non-native</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>g4pE_taskd.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>d</td>\n",
       "      <td>non</td>\n",
       "      <td>non-native</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>g4pE_taske.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>e</td>\n",
       "      <td>non</td>\n",
       "      <td>non-native</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>g4pE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              File  Group Person Task Category Native English  Knowledge  \\\n",
       "0   g0pA_taska.txt      0      A    a      non         native          1   \n",
       "1   g0pA_taskb.txt      0      A    b      cut         native          4   \n",
       "2   g0pA_taskc.txt      0      A    c    light         native          5   \n",
       "3   g0pA_taskd.txt      0      A    d    heavy         native          3   \n",
       "4   g0pA_taske.txt      0      A    e      non         native          4   \n",
       "..             ...    ...    ...  ...      ...            ...        ...   \n",
       "90  g4pE_taska.txt      4      E    a    heavy     non-native          1   \n",
       "91  g4pE_taskb.txt      4      E    b    light     non-native          3   \n",
       "92  g4pE_taskc.txt      4      E    c      cut     non-native          4   \n",
       "93  g4pE_taskd.txt      4      E    d      non     non-native          4   \n",
       "94  g4pE_taske.txt      4      E    e      non     non-native          4   \n",
       "\n",
       "    Difficulty  User  \n",
       "0            1  g0pA  \n",
       "1            3  g0pA  \n",
       "2            3  g0pA  \n",
       "3            4  g0pA  \n",
       "4            3  g0pA  \n",
       "..         ...   ...  \n",
       "90           2  g4pE  \n",
       "91           2  g4pE  \n",
       "92           2  g4pE  \n",
       "93           4  g4pE  \n",
       "94           5  g4pE  \n",
       "\n",
       "[95 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "excel_path = 'c:/Users/dscon/Documents/COURS UM6P/S3/TEXT-MINING/Corpus_Plagiat/Corpus_Plagiat/corpus-final09.xls'\n",
    "sheet_name = 'File list'\n",
    "decision_df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "# Extract User and Task from filename\n",
    "decision_df['User'] = decision_df['File'].apply(getName)\n",
    "decision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes on the 'Task' column\n",
    "merged_df = pd.merge(response_files[['OResponse', 'Task']], users_files, on=['Task'], how='inner')\n",
    "merged_df = pd.merge(merged_df, decision_df, on=['Task', 'User'], how='inner')\n",
    "merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Task</th>\n",
       "      <th>OResponse</th>\n",
       "      <th>UResponse</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pB</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept in object orien...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pC</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>inheritance in object oriented programming is ...</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pD</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance in object oriented programming is ...</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pE</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>g3pC</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In computer science and mathematics, dynamic p...</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>g4pB</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>g4pC</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>g4pD</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>Dynamic programming is a method of providing s...</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>g4pE</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>Dynamic programming is a method for efficient...</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    User Task                                          OResponse  \\\n",
       "0   g0pA    a  In object-oriented programming, inheritance is...   \n",
       "1   g0pB    a  In object-oriented programming, inheritance is...   \n",
       "2   g0pC    a  In object-oriented programming, inheritance is...   \n",
       "3   g0pD    a  In object-oriented programming, inheritance is...   \n",
       "4   g0pE    a  In object-oriented programming, inheritance is...   \n",
       "..   ...  ...                                                ...   \n",
       "90  g3pC    e  In mathematics and computer science, dynamic p...   \n",
       "91  g4pB    e  In mathematics and computer science, dynamic p...   \n",
       "92  g4pC    e  In mathematics and computer science, dynamic p...   \n",
       "93  g4pD    e  In mathematics and computer science, dynamic p...   \n",
       "94  g4pE    e  In mathematics and computer science, dynamic p...   \n",
       "\n",
       "                                            UResponse Category  \n",
       "0   Inheritance is a basic concept of Object-Orien...      non  \n",
       "1   Inheritance is a basic concept in object orien...      non  \n",
       "2   inheritance in object oriented programming is ...    heavy  \n",
       "3   Inheritance in object oriented programming is ...      cut  \n",
       "4   In object-oriented programming, inheritance is...    light  \n",
       "..                                                ...      ...  \n",
       "90  In computer science and mathematics, dynamic p...    light  \n",
       "91  In mathematics and computer science, dynamic p...      cut  \n",
       "92  In mathematics and computer science, dynamic p...    light  \n",
       "93  Dynamic programming is a method of providing s...    heavy  \n",
       "94   Dynamic programming is a method for efficient...      non  \n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = merged_df[['User', 'Task', 'OResponse', 'UResponse', 'Category']]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\2836459194.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[algo_name] = 0.0  # Initialiser chaque colonne avec des scores à 0.0\n",
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\2836459194.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[algo_name] = 0.0  # Initialiser chaque colonne avec des scores à 0.0\n",
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\2836459194.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[algo_name] = 0.0  # Initialiser chaque colonne avec des scores à 0.0\n",
      "C:\\Users\\dscon\\AppData\\Local\\Temp\\ipykernel_9760\\2836459194.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[algo_name] = 0.0  # Initialiser chaque colonne avec des scores à 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Task</th>\n",
       "      <th>OResponse</th>\n",
       "      <th>UResponse</th>\n",
       "      <th>Category</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>N-GRAMM</th>\n",
       "      <th>Set features</th>\n",
       "      <th>Word Order Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.175141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pB</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept in object orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.103627</td>\n",
       "      <td>0.129944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pC</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>inheritance in object oriented programming is ...</td>\n",
       "      <td>heavy</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.310734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pD</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance in object oriented programming is ...</td>\n",
       "      <td>cut</td>\n",
       "      <td>0.132450</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.559322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pE</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>light</td>\n",
       "      <td>0.966419</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.920904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>g3pC</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In computer science and mathematics, dynamic p...</td>\n",
       "      <td>light</td>\n",
       "      <td>0.110737</td>\n",
       "      <td>0.126394</td>\n",
       "      <td>0.238372</td>\n",
       "      <td>0.185053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>g4pB</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>cut</td>\n",
       "      <td>0.615753</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.591954</td>\n",
       "      <td>0.679715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>g4pC</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>light</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.400749</td>\n",
       "      <td>0.549708</td>\n",
       "      <td>0.459075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>g4pD</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>Dynamic programming is a method of providing s...</td>\n",
       "      <td>heavy</td>\n",
       "      <td>0.044361</td>\n",
       "      <td>0.171975</td>\n",
       "      <td>0.326425</td>\n",
       "      <td>0.377224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>g4pE</td>\n",
       "      <td>e</td>\n",
       "      <td>In mathematics and computer science, dynamic p...</td>\n",
       "      <td>Dynamic programming is a method for efficient...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.206406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    User Task                                          OResponse  \\\n",
       "0   g0pA    a  In object-oriented programming, inheritance is...   \n",
       "1   g0pB    a  In object-oriented programming, inheritance is...   \n",
       "2   g0pC    a  In object-oriented programming, inheritance is...   \n",
       "3   g0pD    a  In object-oriented programming, inheritance is...   \n",
       "4   g0pE    a  In object-oriented programming, inheritance is...   \n",
       "..   ...  ...                                                ...   \n",
       "90  g3pC    e  In mathematics and computer science, dynamic p...   \n",
       "91  g4pB    e  In mathematics and computer science, dynamic p...   \n",
       "92  g4pC    e  In mathematics and computer science, dynamic p...   \n",
       "93  g4pD    e  In mathematics and computer science, dynamic p...   \n",
       "94  g4pE    e  In mathematics and computer science, dynamic p...   \n",
       "\n",
       "                                            UResponse Category  Jaro-Winkler  \\\n",
       "0   Inheritance is a basic concept of Object-Orien...      non      0.040767   \n",
       "1   Inheritance is a basic concept in object orien...      non      0.026549   \n",
       "2   inheritance in object oriented programming is ...    heavy      0.020779   \n",
       "3   Inheritance in object oriented programming is ...      cut      0.132450   \n",
       "4   In object-oriented programming, inheritance is...    light      0.966419   \n",
       "..                                                ...      ...           ...   \n",
       "90  In computer science and mathematics, dynamic p...    light      0.110737   \n",
       "91  In mathematics and computer science, dynamic p...      cut      0.615753   \n",
       "92  In mathematics and computer science, dynamic p...    light      0.043729   \n",
       "93  Dynamic programming is a method of providing s...    heavy      0.044361   \n",
       "94   Dynamic programming is a method for efficient...      non      0.030061   \n",
       "\n",
       "     N-GRAMM  Set features  Word Order Similarity  \n",
       "0   0.010453      0.080645               0.175141  \n",
       "1   0.006431      0.103627               0.129944  \n",
       "2   0.066929      0.191860               0.310734  \n",
       "3   0.373737      0.533333               0.559322  \n",
       "4   0.897143      0.909091               0.920904  \n",
       "..       ...           ...                    ...  \n",
       "90  0.126394      0.238372               0.185053  \n",
       "91  0.540741      0.591954               0.679715  \n",
       "92  0.400749      0.549708               0.459075  \n",
       "93  0.171975      0.326425               0.377224  \n",
       "94  0.030387      0.140426               0.206406  \n",
       "\n",
       "[95 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Prétraitement du texte\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def ngram_similarity(str1, str2, n=2):\n",
    "    str1 = str1.split()\n",
    "    str2 = str2.split()\n",
    "    ngrams1 = {tuple(str1[i:i+n]) for i in range(len(str1) - n + 1)}\n",
    "    ngrams2 = {tuple(str2[i:i+n]) for i in range(len(str2) - n + 1)}\n",
    "    intersection = ngrams1.intersection(ngrams2)\n",
    "    return len(intersection) / float(len(ngrams1.union(ngrams2)))\n",
    "\n",
    "def set_features_similarity(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    intersection = set1.intersection(set2)\n",
    "    return len(intersection) / float(len(set1.union(set2)))\n",
    "\n",
    "def word_order_similarity(str1, str2):\n",
    "    words1 = str1.split()\n",
    "    words2 = str2.split()\n",
    "    matching_words = sum(1 for word in words1 if word in words2)\n",
    "    return matching_words / float(max(len(words1), len(words2)))\n",
    "\n",
    "def apply_syntaxic_similarity(df, algorithms):\n",
    "    # Pour chaque algorithme, ajouter une colonne vide dans le DataFrame\n",
    "    for algo_name in algorithms.keys():\n",
    "        df[algo_name] = 0.0  # Initialiser chaque colonne avec des scores à 0.0\n",
    "\n",
    "    # Appliquer chaque algorithme de similarité aux lignes du DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        response = preprocess_text(row['UResponse'])\n",
    "        original_response = preprocess_text(row['OResponse'])\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            similarity_score = algo(response, original_response)\n",
    "            df.at[index, algo_name] = similarity_score  # Ajouter le score dans la colonne correspondante\n",
    "\n",
    "    return df\n",
    "\n",
    "def classify_similarity(similarity):\n",
    "    if similarity >= 0.75:\n",
    "        return 'cut'      \n",
    "    elif similarity >= 0.50:\n",
    "        return 'heavy'\n",
    "    elif similarity >= 0.25:\n",
    "        return 'light'\n",
    "    else:\n",
    "        return 'non'\n",
    "    \n",
    "# Définir les algorithmes de similarité\n",
    "algorithms = {\n",
    "    'Jaro-Winkler': jaro_winkler_similarity,\n",
    "    'N-GRAMM': ngram_similarity,\n",
    "    'Set features': set_features_similarity,\n",
    "    'Word Order Similarity': word_order_similarity,\n",
    "}\n",
    "\n",
    "# Appliquer la fonction sur votre DataFrame\n",
    "df_syntaxic = apply_syntaxic_similarity(new_df, algorithms)\n",
    "df_syntaxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Task</th>\n",
       "      <th>OResponse</th>\n",
       "      <th>UResponse</th>\n",
       "      <th>Category</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>N-GRAMM</th>\n",
       "      <th>Set features</th>\n",
       "      <th>Word Order Similarity</th>\n",
       "      <th>Category_JARO</th>\n",
       "      <th>Category_NGRAM</th>\n",
       "      <th>Category_Set</th>\n",
       "      <th>Category_WordOrder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pB</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept in object orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.103627</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pC</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>inheritance in object oriented programming is ...</td>\n",
       "      <td>heavy</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.310734</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pD</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance in object oriented programming is ...</td>\n",
       "      <td>cut</td>\n",
       "      <td>0.132450</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>non</td>\n",
       "      <td>cut</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pE</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>light</td>\n",
       "      <td>0.966419</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>heavy</td>\n",
       "      <td>heavy</td>\n",
       "      <td>heavy</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User Task                                          OResponse  \\\n",
       "0  g0pA    a  In object-oriented programming, inheritance is...   \n",
       "1  g0pB    a  In object-oriented programming, inheritance is...   \n",
       "2  g0pC    a  In object-oriented programming, inheritance is...   \n",
       "3  g0pD    a  In object-oriented programming, inheritance is...   \n",
       "4  g0pE    a  In object-oriented programming, inheritance is...   \n",
       "\n",
       "                                           UResponse Category  Jaro-Winkler  \\\n",
       "0  Inheritance is a basic concept of Object-Orien...      non      0.040767   \n",
       "1  Inheritance is a basic concept in object orien...      non      0.026549   \n",
       "2  inheritance in object oriented programming is ...    heavy      0.020779   \n",
       "3  Inheritance in object oriented programming is ...      cut      0.132450   \n",
       "4  In object-oriented programming, inheritance is...    light      0.966419   \n",
       "\n",
       "    N-GRAMM  Set features  Word Order Similarity Category_JARO Category_NGRAM  \\\n",
       "0  0.010453      0.080645               0.175141           non            non   \n",
       "1  0.006431      0.103627               0.129944           non            non   \n",
       "2  0.066929      0.191860               0.310734           non            non   \n",
       "3  0.373737      0.533333               0.559322           non            cut   \n",
       "4  0.897143      0.909091               0.920904         heavy          heavy   \n",
       "\n",
       "  Category_Set Category_WordOrder  \n",
       "0          non                non  \n",
       "1          non                non  \n",
       "2          non                cut  \n",
       "3        light              light  \n",
       "4        heavy              heavy  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classer les similarités\n",
    "df_syntaxic['Category_JARO'] = df_syntaxic['Jaro-Winkler'].apply(classify_similarity)\n",
    "df_syntaxic['Category_NGRAM'] = df_syntaxic['N-GRAMM'].apply(classify_similarity)\n",
    "df_syntaxic['Category_Set'] = df_syntaxic['Set features'].apply(classify_similarity)\n",
    "df_syntaxic['Category_WordOrder'] = df_syntaxic['Word Order Similarity'].apply(classify_similarity)\n",
    "df_syntaxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaro-Winkler Accuracy: 44.21052631578947\n",
      "N-GRAMM Accuracy: 48.421052631578945\n",
      "Set features Accuracy: 53.68421052631579\n",
      "Word Order Similarity Accuracy: 51.578947368421055\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Jaro-Winkler Accuracy:\", sum(df_syntaxic['Category_JARO'] == df_syntaxic['Category'])/ len(new_df) *100)\n",
    "print(\"N-GRAMM Accuracy:\", sum(df_syntaxic['Category_NGRAM'] == df_syntaxic['Category'])/ len(new_df) * 100)\n",
    "print(\"Set features Accuracy:\", sum(df_syntaxic['Category_Set'] == df_syntaxic['Category'])/ len(new_df) * 100)\n",
    "print(\"Word Order Similarity Accuracy:\", sum(df_syntaxic['Category_WordOrder'] == df_syntaxic['Category'])/ len(new_df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate n-grams\n",
    "def ngrams(text, n=3):\n",
    "    words = text.split()\n",
    "    return set([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n",
    "\n",
    "# Calculate the Jaccard index\n",
    "def jaccard_index(doc1, doc2, n=3):\n",
    "    ngrams_doc1 = ngrams(doc1, n)\n",
    "    ngrams_doc2 = ngrams(doc2, n)\n",
    "    intersection = ngrams_doc1.intersection(ngrams_doc2)\n",
    "    union = ngrams_doc1.union(ngrams_doc2)\n",
    "    return len(intersection) / len(union) if len(union) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "# Classification based on the similarity score\n",
    "def classify_similarity(similarity_score):\n",
    "    if similarity_score < 0.2:\n",
    "        return 'non'\n",
    "    elif 0.2 <= similarity_score < 0.5:\n",
    "        return 'cut'\n",
    "    elif 0.5 <= similarity_score < 0.75:\n",
    "        return 'light'\n",
    "    else:\n",
    "        return 'heavy'\n",
    "\n",
    "# Compute semantic similarity for the DataFrame\n",
    "def compute_semantic_similarity(df):\n",
    "    semantic_results = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        doc1 = df['OResponse'][i]\n",
    "        doc2 = df['UResponse'][i]\n",
    "        \n",
    "        semantic = SemanticDistanceDocs(doc1, doc2)\n",
    "        similarity_class = classify_similarity(semantic)\n",
    "\n",
    "        semantic_results.append({\n",
    "            'Semantic_Similarity': semantic,\n",
    "            'Semantic_Category': similarity_class\n",
    "        })\n",
    "\n",
    "    semantic_df = pd.DataFrame(semantic_results)\n",
    "    df = pd.concat([df.reset_index(drop=True), semantic_df], axis=1)\n",
    "    return df\n",
    "\n",
    "result = compute_semantic_similarity(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User', 'Task', 'OResponse', 'UResponse', 'Category', 'Jaro-Winkler',\n",
       "       'N-GRAMM', 'Set features', 'Word Order Similarity', 'Category_JARO',\n",
       "       'Category_NGRAM', 'Category_Set', 'Category_WordOrder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User', 'Task', 'OResponse', 'UResponse', 'Category', 'Jaro-Winkler',\n",
       "       'N-GRAMM', 'Set features', 'Word Order Similarity', 'Category_JARO',\n",
       "       'Category_NGRAM', 'Category_Set', 'Category_WordOrder',\n",
       "       'Semantic_Similarity', 'Semantic_Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Task</th>\n",
       "      <th>OResponse</th>\n",
       "      <th>UResponse</th>\n",
       "      <th>Category</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>N-GRAMM</th>\n",
       "      <th>Set features</th>\n",
       "      <th>Word Order Similarity</th>\n",
       "      <th>Category_JARO</th>\n",
       "      <th>Category_NGRAM</th>\n",
       "      <th>Category_Set</th>\n",
       "      <th>Category_WordOrder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pB</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance is a basic concept in object orien...</td>\n",
       "      <td>non</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.103627</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pC</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>inheritance in object oriented programming is ...</td>\n",
       "      <td>heavy</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.310734</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pD</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>Inheritance in object oriented programming is ...</td>\n",
       "      <td>cut</td>\n",
       "      <td>0.132450</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>non</td>\n",
       "      <td>light</td>\n",
       "      <td>heavy</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pE</td>\n",
       "      <td>a</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>In object-oriented programming, inheritance is...</td>\n",
       "      <td>light</td>\n",
       "      <td>0.966419</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>cut</td>\n",
       "      <td>cut</td>\n",
       "      <td>cut</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User Task                                          OResponse  \\\n",
       "0  g0pA    a  In object-oriented programming, inheritance is...   \n",
       "1  g0pB    a  In object-oriented programming, inheritance is...   \n",
       "2  g0pC    a  In object-oriented programming, inheritance is...   \n",
       "3  g0pD    a  In object-oriented programming, inheritance is...   \n",
       "4  g0pE    a  In object-oriented programming, inheritance is...   \n",
       "\n",
       "                                           UResponse Category  Jaro-Winkler  \\\n",
       "0  Inheritance is a basic concept of Object-Orien...      non      0.040767   \n",
       "1  Inheritance is a basic concept in object orien...      non      0.026549   \n",
       "2  inheritance in object oriented programming is ...    heavy      0.020779   \n",
       "3  Inheritance in object oriented programming is ...      cut      0.132450   \n",
       "4  In object-oriented programming, inheritance is...    light      0.966419   \n",
       "\n",
       "    N-GRAMM  Set features  Word Order Similarity Category_JARO Category_NGRAM  \\\n",
       "0  0.010453      0.080645               0.175141           non            non   \n",
       "1  0.006431      0.103627               0.129944           non            non   \n",
       "2  0.066929      0.191860               0.310734           non            non   \n",
       "3  0.373737      0.533333               0.559322           non          light   \n",
       "4  0.897143      0.909091               0.920904           cut            cut   \n",
       "\n",
       "  Category_Set Category_WordOrder  \n",
       "0          non                non  \n",
       "1          non                non  \n",
       "2          non              light  \n",
       "3        heavy              heavy  \n",
       "4          cut                cut  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Semantic Similarity: 20.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of Semantic Similarity: {sum(new_df[\"Category\"] == result[\"Semantic_Category\"])/len(new_df) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of Semantic Similarity: 20.0%**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SnE8UK8y0yYy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
